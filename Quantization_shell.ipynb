{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanaySoker/Quantization/blob/main/Quantization_shell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUAsfJUSDr_e"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\n",
        "%cd lm-evaluation-harness"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "zosgJ8bm4Irw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "rhImwAuEhERx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ./lm_eval/models/qu.py"
      ],
      "metadata": {
        "id": "TJRb34raECnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./lm_eval/models/qu.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd.function import InplaceFunction\n",
        "import time\n",
        "\n",
        "from . import gpt2\n",
        "from . import gpt3\n",
        "from . import dummy\n",
        "\n",
        "# Change here:\n",
        "class qu(gpt2.HFLM):\n",
        "    def __init__(self, B, *args, **kwargs):\n",
        "        super(qu, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.E_bits = 2\n",
        "        self.B = B\n",
        "        self.m = 5\n",
        "\n",
        "        self.E = 2**self.E_bits\n",
        "\n",
        "        self.quantizeFwd = False\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        if self.quantizeFwd:\n",
        "            w_q = FPQuantizeSawb.apply(self.weight, self.B, self.E, self.m)\n",
        "\n",
        "            if torch.min(input) < 0:\n",
        "                self.QnA = -2 ** (self.abits - 1)\n",
        "\n",
        "            qinput = FPQuantizeSawb.apply(input, self.B, self.E, self.m)\n",
        "            # all\n",
        "            output = F.conv2d(qinput, w_q, self.bias, self.stride,\n",
        "                              self.padding, self.dilation, self.groups)\n",
        "\n",
        "        else:\n",
        "            output = F.conv2d(input, self.weight, self.bias, self.stride,\n",
        "                              self.padding, self.dilation, self.groups)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class FPQuantizeSawb(InplaceFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, B, E, m):\n",
        "        sign = (input.__ge__(0).int() - 0.5) * 2\n",
        "        abs_input = input * sign\n",
        "\n",
        "        Max = (2 - 2 ** (-m)) * (2 ** (E - B - 1))\n",
        "        abs_input = torch.clip(abs_input, max=Max)\n",
        "\n",
        "        abs_input_without_0 = torch.clip(abs_input, min=2 ** (1 - B))\n",
        "        exp_range = torch.log2(abs_input_without_0)\n",
        "        exp_range = exp_range + B\n",
        "        exp_range = exp_range.int()\n",
        "        exp_range = exp_range - B\n",
        "\n",
        "        upper_bound = 2 ** (exp_range + 1)\n",
        "        min_mask = torch.ge(exp_range, 1.5 - B).int()\n",
        "        lower_bound = 2 ** exp_range\n",
        "        lower_bound = lower_bound * min_mask\n",
        "        num_of_value = 2 ** m\n",
        "        num_of_values = (2 - min_mask) * num_of_value\n",
        "\n",
        "        abs_input = abs_input - lower_bound\n",
        "        abs_input = abs_input * num_of_values\n",
        "        abs_input = abs_input / (upper_bound - lower_bound)\n",
        "        abs_input = abs_input + 0.5\n",
        "        abs_input = abs_input.int()\n",
        "        abs_input = abs_input * (upper_bound - lower_bound)\n",
        "        abs_input = abs_input / num_of_values\n",
        "        abs_input = abs_input + lower_bound\n",
        "        return abs_input * sign\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # straight-through estimator\n",
        "        grad_input = grad_output\n",
        "        return grad_input, None, None, None, None"
      ],
      "metadata": {
        "id": "w3d5Z41tD0Oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b07361b-7799-4c1a-fdd4-c0d5c6aee967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./lm_eval/models/qu.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ./lm_eval/models/__init__.py"
      ],
      "metadata": {
        "id": "2Pa6TJCizTj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./lm_eval/models/__init__.py\n",
        "from . import gpt2\n",
        "from . import gpt3\n",
        "from . import dummy\n",
        "\n",
        "from . import qu\n",
        "\n",
        "MODEL_REGISTRY = {\n",
        "    \"qu\": qu.qu,\n",
        "}\n",
        "\n",
        "\n",
        "def get_model(model_name):\n",
        "    return MODEL_REGISTRY[model_name]"
      ],
      "metadata": {
        "id": "RdG0Tlw1znrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42516d98-0f04-4876-bcbd-f4e695f055c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./lm_eval/models/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ./main.py"
      ],
      "metadata": {
        "id": "yjDa1zY51wxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./main.py\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import fnmatch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Change here:\n",
        "FROM, TO = -6, 20\n",
        "\n",
        "\n",
        "from lm_eval import tasks, evaluator\n",
        "\n",
        "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "class MultiChoice:\n",
        "    def __init__(self, choices):\n",
        "        self.choices = choices\n",
        "\n",
        "    # Simple wildcard support (linux filename patterns)\n",
        "    def __contains__(self, values):\n",
        "        for value in values.split(\",\"):\n",
        "            if len(fnmatch.filter(self.choices, value)) == 0:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def __iter__(self):\n",
        "        for choice in self.choices:\n",
        "            yield choice\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", required=True)\n",
        "    parser.add_argument(\"--model_args\", default=\"\")\n",
        "    parser.add_argument(\"--tasks\", default=None, choices=MultiChoice(tasks.ALL_TASKS))\n",
        "    parser.add_argument(\"--provide_description\", action=\"store_true\")\n",
        "    parser.add_argument(\"--num_fewshot\", type=int, default=0)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=None)\n",
        "    parser.add_argument(\"--device\", type=str, default=None)\n",
        "    parser.add_argument(\"--output_path\", default=None)\n",
        "    parser.add_argument(\"--limit\", type=int, default=None)\n",
        "    parser.add_argument(\"--no_cache\", action=\"store_true\")\n",
        "    parser.add_argument(\"--decontamination_ngrams_path\", default=None)\n",
        "    parser.add_argument(\"--description_dict_path\", default=None)\n",
        "    parser.add_argument(\"--check_integrity\", action=\"store_true\")\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "# Returns a list containing all values of the source_list that\n",
        "# match at least one of the patterns\n",
        "def pattern_match(patterns, source_list):\n",
        "    task_names = set()\n",
        "    for pattern in patterns:\n",
        "        for matching in fnmatch.filter(source_list, pattern):\n",
        "            task_names.add(matching)\n",
        "    return list(task_names)\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    assert not args.provide_description  # not implemented\n",
        "\n",
        "    if args.limit:\n",
        "        print(\n",
        "            \"WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\"\n",
        "        )\n",
        "\n",
        "    if args.tasks is None:\n",
        "        task_names = tasks.ALL_TASKS\n",
        "    else:\n",
        "        task_names = pattern_match(args.tasks.split(\",\"), tasks.ALL_TASKS)\n",
        "\n",
        "    print(f\"Selected Tasks: {task_names}\")\n",
        "\n",
        "    description_dict = {}\n",
        "    if args.description_dict_path:\n",
        "        with open(args.description_dict_path, \"r\") as f:\n",
        "            description_dict = json.load(f)\n",
        "\n",
        "    R = dict()\n",
        "\n",
        "    for i in range(FROM, TO):\n",
        "        print(f\"B = {i}:\")\n",
        "        results = evaluator.simple_evaluate(\n",
        "            model=args.model,\n",
        "            model_args=f\"B={i}\",\n",
        "            tasks=task_names,\n",
        "            num_fewshot=args.num_fewshot,\n",
        "            batch_size=args.batch_size,\n",
        "            device=args.device,\n",
        "            no_cache=args.no_cache,\n",
        "            limit=args.limit,\n",
        "            description_dict=description_dict,\n",
        "            decontamination_ngrams_path=args.decontamination_ngrams_path,\n",
        "            check_integrity=args.check_integrity,\n",
        "        )\n",
        "        results = results['results']\n",
        "        for K in results.keys():\n",
        "            for k in results[K].keys():\n",
        "                if \"_stderr\" not in k:\n",
        "                    if f\"{K}: {k}\" not in R.keys():\n",
        "                        R[f\"{K}: {k}\"] = []\n",
        "                    R[f\"{K}: {k}\"].append(results[K][k])\n",
        "                    print(f\"{K}: {k}: {results[K][k]}\")\n",
        "\n",
        "    _keys = list(R.keys())\n",
        "    for k in _keys:\n",
        "        print(f\"{k}: {R[k]}\")\n",
        "        plt.plot(range(FROM, TO), R[k], label=k)\n",
        "    plt.legend(_keys)\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bYLrqcoJ16TN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7203e76f-404c-436f-96a3-5c2d2d1b8ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change here - the tasks:"
      ],
      "metadata": {
        "id": "L_T3vcFH96dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "\t--model qu \\\n",
        "\t--device 0 \\\n",
        "\t--tasks hellaswag"
      ],
      "metadata": {
        "id": "aFwSgJpC1_LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y7pZWScF2t_o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}