{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanaySoker/Quantization/blob/main/qunt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUAsfJUSDr_e",
        "outputId": "51cfaabb-bbdf-4e06-fd80-bc01c66f4417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lm-evaluation-harness'...\n",
            "remote: Enumerating objects: 8101, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 8101 (delta 46), reused 52 (delta 35), pack-reused 8021\u001b[K\n",
            "Receiving objects: 100% (8101/8101), 9.48 MiB | 24.90 MiB/s, done.\n",
            "Resolving deltas: 100% (5237/5237), done.\n",
            "/content/lm-evaluation-harness\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\n",
        "%cd lm-evaluation-harness"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zozoCJ9rbh2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VImcJVTkL1Ge",
        "outputId": "2e06ec7c-9ee2-41dd-e48b-992332f38449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/lm-evaluation-harness (from -r requirements.txt (line 1))\n",
            "Collecting bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt\n",
            "  Downloading https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip\n",
            "\u001b[K     \\ 16.5 MB 97 kB/s\n",
            "\u001b[?25hCollecting datasets>=2.0.0\n",
            "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "\u001b[K     |████████████████████████████████| 441 kB 29.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.1 in /usr/local/lib/python3.7/dist-packages (from lm-eval==0.2.0->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from lm-eval==0.2.0->-r requirements.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from lm-eval==0.2.0->-r requirements.txt (line 1)) (1.12.1+cu113)\n",
            "Collecting transformers>=4.1\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 66.4 MB/s \n",
            "\u001b[?25hCollecting sqlitedict==1.6.0\n",
            "  Downloading sqlitedict-1.6.0.tar.gz (29 kB)\n",
            "Collecting pytablewriter==0.58.0\n",
            "  Downloading pytablewriter-0.58.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting sacrebleu==1.5.0\n",
            "  Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting rouge-score==0.0.4\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Collecting pycountry==20.7.3\n",
            "  Downloading pycountry-20.7.3.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from lm-eval==0.2.0->-r requirements.txt (line 1)) (2.8.4)\n",
            "Collecting lm_dataformat==0.0.20\n",
            "  Downloading lm_dataformat-0.0.20-py3-none-any.whl (5.8 kB)\n",
            "Collecting pybind11==2.6.2\n",
            "  Downloading pybind11-2.6.2-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 57.2 MB/s \n",
            "\u001b[?25hCollecting tqdm-multiprocess==0.0.11\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Collecting zstandard==0.15.2\n",
            "  Downloading zstandard-0.15.2-cp37-cp37m-manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 50.4 MB/s \n",
            "\u001b[?25hCollecting jsonlines==2.0.0\n",
            "  Downloading jsonlines-2.0.0-py3-none-any.whl (6.3 kB)\n",
            "Collecting mock==4.0.3\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting openai==0.6.4\n",
            "  Downloading openai-0.6.4.tar.gz (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 64.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba==0.42.1 in /usr/local/lib/python3.7/dist-packages (from lm-eval==0.2.0->-r requirements.txt (line 1)) (0.42.1)\n",
            "Collecting nagisa==0.2.7\n",
            "  Downloading nagisa-0.2.7-cp37-cp37m-manylinux1_x86_64.whl (21.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (2.9.2)\n",
            "Collecting tf-slim>=1.1\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 58.9 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 49.4 MB/s \n",
            "\u001b[?25hCollecting ujson\n",
            "  Downloading ujson-5.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting DyNet\n",
            "  Downloading dyNET-2.1.2-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nagisa==0.2.7->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai==0.6.4->lm-eval==0.2.0->-r requirements.txt (line 1)) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai==0.6.4->lm-eval==0.2.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Collecting mbstrdecoder<2,>=1.0.0\n",
            "  Downloading mbstrdecoder-1.1.1-py3-none-any.whl (7.7 kB)\n",
            "Collecting pathvalidate<3,>=2.3.0\n",
            "  Downloading pathvalidate-2.5.2-py3-none-any.whl (20 kB)\n",
            "Collecting tabledata<2,>=1.1.3\n",
            "  Downloading tabledata-1.3.0-py3-none-any.whl (11 kB)\n",
            "Collecting msgfy<1,>=0.1.0\n",
            "  Downloading msgfy-0.2.0-py3-none-any.whl (4.3 kB)\n",
            "Collecting typepy[datetime]<2,>=1.1.1\n",
            "  Downloading typepy-1.3.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter==0.58.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (57.4.0)\n",
            "Collecting DataProperty<2,>=0.50.0\n",
            "  Downloading DataProperty-0.55.0-py3-none-any.whl (26 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5\n",
            "  Downloading tcolorpy-0.1.2-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score==0.0.4->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score==0.0.4->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.7)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.8.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 52.3 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 76.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (6.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 77.2 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (2022.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter==0.58.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai==0.6.4->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai==0.6.4->lm-eval==0.2.0->-r requirements.txt (line 1)) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai==0.6.4->lm-eval==0.2.0->-r requirements.txt (line 1)) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 77.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.1->lm-eval==0.2.0->-r requirements.txt (line 1)) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.7/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter==0.58.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (2022.6)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter==0.58.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from DyNet->nagisa==0.2.7->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.29.32)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=2.0.0->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.10.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 72.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (2.9.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (2.9.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.27.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (14.0.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.50.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.38.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.2.0->-r requirements.txt (line 1)) (3.2.2)\n",
            "Building wheels for collected packages: bleurt, openai, pycountry, sqlitedict\n",
            "  Building wheel for bleurt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bleurt: filename=BLEURT-0.0.2-py3-none-any.whl size=16454022 sha256=0d3c3d684626f53f906459983193643a433706aa06550c6a3ce0d63cc17fdea5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ubw9qfj9/wheels/e4/25/b5/d3b124fc2717a794a8dcb176fac9a01bec46e9f3ff0dbf5d37\n",
            "  Building wheel for openai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.6.4-py3-none-any.whl size=172214 sha256=f8e7415f51c40a7a2a465a0d37e7e51dc604a837f0b7e2f4a25674df35b1f919\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/c4/02/aa519fe2aaf97a9bba197a8585182c8c07802760351afdb64a\n",
            "  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746883 sha256=da580d123e1bfbcad3ec5460a5c73c4a8f24e3170824bed2b9d24b1cb43da8c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/e8/3f/120ccc1ff7541c108bc5d656e2a14c39da0d824653b62284c6\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-py3-none-any.whl size=14712 sha256=30392488cf045e94f1aa38f64c69c20a8aa52b7562c5465bd2f69c9807f133da\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/dd/2c/0a57aadf6a7f26bec0af66d742c50af74d11967780f0bb7a7d\n",
            "Successfully built bleurt openai pycountry sqlitedict\n",
            "Installing collected packages: urllib3, mbstrdecoder, typepy, dill, DataProperty, zstandard, xxhash, ujson, tokenizers, tf-slim, tcolorpy, tabledata, sentencepiece, responses, portalocker, pathvalidate, multiprocess, msgfy, jsonlines, huggingface-hub, DyNet, colorama, transformers, tqdm-multiprocess, sqlitedict, sacrebleu, rouge-score, pytablewriter, pycountry, pybind11, openai, nagisa, mock, lm-dataformat, datasets, bleurt, lm-eval\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Running setup.py develop for lm-eval\n",
            "Successfully installed DataProperty-0.55.0 DyNet-2.1.2 bleurt-0.0.2 colorama-0.4.6 datasets-2.6.1 dill-0.3.5.1 huggingface-hub-0.10.1 jsonlines-2.0.0 lm-dataformat-0.0.20 lm-eval-0.2.0 mbstrdecoder-1.1.1 mock-4.0.3 msgfy-0.2.0 multiprocess-0.70.13 nagisa-0.2.7 openai-0.6.4 pathvalidate-2.5.2 portalocker-2.6.0 pybind11-2.6.2 pycountry-20.7.3 pytablewriter-0.58.0 responses-0.18.0 rouge-score-0.0.4 sacrebleu-1.5.0 sentencepiece-0.1.97 sqlitedict-1.6.0 tabledata-1.3.0 tcolorpy-0.1.2 tf-slim-1.1.0 tokenizers-0.13.2 tqdm-multiprocess-0.0.11 transformers-4.24.0 typepy-1.3.0 ujson-5.5.0 urllib3-1.25.11 xxhash-3.1.0 zstandard-0.15.2\n"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhImwAuEhERx",
        "outputId": "b0ad283a-306a-4901-f47c-ee84e3de4c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjSHcd1wb5Lq",
        "outputId": "029ce58e-f3f2-4d69-e940-876cddd36237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/alpa-projects/alpa.git\n",
            "  Cloning https://github.com/alpa-projects/alpa.git to /tmp/pip-req-build-ztn16dd6\n",
            "  Running command git clone -q https://github.com/alpa-projects/alpa.git /tmp/pip-req-build-ztn16dd6\n",
            "  Running command git submodule update --init --recursive -q\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement as (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for as\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/alpa-projects/alpa.git as alpa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5h7BzV6c5t7",
        "outputId": "2a2d24d8-9513-42f6-a5d6-a5473c66f59b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./lm_eval/evaluator.py\n",
        "import collections\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "import lm_eval.metrics\n",
        "import lm_eval.models\n",
        "import lm_eval.tasks\n",
        "import lm_eval.base\n",
        "from lm_eval.utils import positional_deprecated, run_task_tests\n",
        "\n",
        "\n",
        "@positional_deprecated\n",
        "def simple_evaluate(\n",
        "    model,\n",
        "    model_args=None,\n",
        "    tasks=[],\n",
        "    num_fewshot=0,\n",
        "    batch_size=None,\n",
        "    device=None,\n",
        "    no_cache=False,\n",
        "    limit=None,\n",
        "    bootstrap_iters=100000,\n",
        "    description_dict=None,\n",
        "    check_integrity=False,\n",
        "    decontamination_ngrams_path=None,\n",
        "):\n",
        "\n",
        "    \"\"\"Instantiate and evaluate a model on a list of tasks.\n",
        "    :param model: Union[str, LM]\n",
        "        Name of model or LM object, see lm_eval.models.get_model\n",
        "    :param model_args: Optional[str]\n",
        "        String arguments for each model class, see LM.create_from_arg_string.\n",
        "        Ignored if `model` argument is a LM object.\n",
        "    :param tasks: list[Union[str, Task]]\n",
        "        List of task names or Task objects. Task objects will be taken to have name task.EVAL_HARNESS_NAME if defined and type(task).__name__ otherwise.\n",
        "    :param num_fewshot: int\n",
        "        Number of examples in few-shot context\n",
        "    :param batch_size: int, optional\n",
        "        Batch size for model\n",
        "    :param device: str, optional\n",
        "        PyTorch device (e.g. \"cpu\" or \"cuda:0\") for running models\n",
        "    :param no_cache: bool\n",
        "        Whether or not to cache\n",
        "    :param limit: int, optional\n",
        "        Limit the number of examples per task (only use this for testing)\n",
        "    :param bootstrap_iters:\n",
        "        Number of iterations for bootstrap statistics\n",
        "    :param description_dict: dict[str, str]\n",
        "        Dictionary of custom task descriptions of the form: `task_name: description`\n",
        "    :param check_integrity: bool\n",
        "        Whether to run the relevant part of the test suite for the tasks\n",
        "    :return\n",
        "        Dictionary of results\n",
        "    \"\"\"\n",
        "    random.seed(1234)\n",
        "    np.random.seed(1234)\n",
        "\n",
        "    assert tasks != [], \"No tasks specified\"\n",
        "\n",
        "    if isinstance(model, str):\n",
        "        if model_args is None:\n",
        "            model_args = \"\"\n",
        "        lm = lm_eval.models.get_model(model).create_from_arg_string(\n",
        "            model_args, {\"batch_size\": batch_size, \"device\": device}\n",
        "        )\n",
        "    else:\n",
        "        assert isinstance(model, lm_eval.base.LM)\n",
        "        lm = model\n",
        "\n",
        "    if not no_cache:\n",
        "        lm = lm_eval.base.CachingLM(\n",
        "            lm,\n",
        "            \"lm_cache/\"\n",
        "            + model\n",
        "            + \"_\"\n",
        "            + model_args.replace(\"=\", \"-\").replace(\",\", \"_\").replace(\"/\", \"-\")\n",
        "            + \".db\",\n",
        "        )\n",
        "\n",
        "    task_dict = lm_eval.tasks.get_task_dict(tasks)\n",
        "\n",
        "    if check_integrity:\n",
        "        run_task_tests(task_list=tasks)\n",
        "\n",
        "    results = evaluate(\n",
        "        lm=lm,\n",
        "        task_dict=task_dict,\n",
        "        num_fewshot=num_fewshot,\n",
        "        limit=limit,\n",
        "        bootstrap_iters=bootstrap_iters,\n",
        "        description_dict=description_dict,\n",
        "        decontamination_ngrams_path=decontamination_ngrams_path,\n",
        "    )\n",
        "\n",
        "    # add info about the model and few shot config\n",
        "    results[\"config\"] = {\n",
        "        \"model\": model,\n",
        "        \"model_args\": model_args,\n",
        "        \"num_fewshot\": num_fewshot,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"device\": device,\n",
        "        \"no_cache\": no_cache,\n",
        "        \"limit\": limit,\n",
        "        \"bootstrap_iters\": bootstrap_iters,\n",
        "        \"description_dict\": description_dict,\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "decontaminate_suffix = \"_decontaminate\"\n",
        "\n",
        "\n",
        "@positional_deprecated\n",
        "def evaluate(\n",
        "    lm,\n",
        "    task_dict,\n",
        "    provide_description=None,\n",
        "    num_fewshot=0,\n",
        "    limit=None,\n",
        "    bootstrap_iters=100000,\n",
        "    description_dict=None,\n",
        "    decontamination_ngrams_path=None,\n",
        "):\n",
        "    \"\"\"Instantiate and evaluate a model on a list of tasks.\n",
        "    :param lm: obj\n",
        "        Language Model\n",
        "    :param task_dict: dict[str, Task]\n",
        "        Dictionary of tasks. Tasks will be taken to have name task.EVAL_HARNESS_NAME if defined and type(task).__name__ otherwise.\n",
        "    :param provide_description: bool\n",
        "        Not implemented, and this option is deprecated and will be removed in a future version in favor of a different description providing method\n",
        "    :param num_fewshot: int\n",
        "        Number of examples in few-shot context\n",
        "    :param limit: int, optional\n",
        "        Limit the number of examples per task (only use this for testing)\n",
        "    :param bootstrap_iters:\n",
        "        Number of iterations for bootstrap statistics\n",
        "    :param description_dict: dict[str, str]\n",
        "        Dictionary of custom task descriptions of the form: `task_name: description`\n",
        "    :return\n",
        "        Dictionary of results\n",
        "    \"\"\"\n",
        "    # TODO: completely refactor this entire function to not be a huge mess, ideally breaking it down into smaller pieces\n",
        "\n",
        "    # TODO: todo: implement proper description-providing system\n",
        "    assert not provide_description  # not implemented.\n",
        "    if provide_description is not None:\n",
        "        # nudge people to not specify it at all\n",
        "        print(\n",
        "            \"WARNING: provide_description is deprecated and will be removed in a future version in favor of description_dict\"\n",
        "        )\n",
        "\n",
        "\n",
        "    decontaminate = decontamination_ngrams_path is not None\n",
        "\n",
        "    task_dict_items = [\n",
        "        (name, task)\n",
        "        for name, task in task_dict.items()\n",
        "        if (task.has_validation_docs() or task.has_test_docs())\n",
        "    ]\n",
        "\n",
        "    results = collections.defaultdict(dict)\n",
        "    versions = collections.defaultdict(dict)\n",
        "\n",
        "    requests = collections.defaultdict(list)\n",
        "    requests_origin = collections.defaultdict(list)\n",
        "\n",
        "    overlaps = collections.defaultdict(list)  # {task_name: contaminated_docs}\n",
        "\n",
        "    # If we ever run into issues where the eval tasks don't fit in memory and we can't afford a machine with bigger\n",
        "    # memory, we can always modify this plumbing to support that, but I didn't want to include it just yet because\n",
        "    # over-engineering is bad (or we could make it write the requests to disk and then read them back out again\n",
        "    #  - probably using an sqlite db because of all the moving parts we have\n",
        "\n",
        "    # TODO: we need unit tests & sanity checks or something to ensure that the return of `validation_docs` is stable\n",
        "    docs = {}\n",
        "\n",
        "    docs_for_decontamination = collections.defaultdict(list)\n",
        "\n",
        "    # get lists of each type of request\n",
        "    for task_name, task in task_dict_items:\n",
        "        versions[task_name] = task.VERSION\n",
        "        # default to test doc, fall back to val doc if validation unavailable\n",
        "        # TODO: the test-fallback-to-val system isn't final, we should revisit it at some point\n",
        "        if task.has_test_docs():\n",
        "            task_doc_func = task.test_docs\n",
        "            task_set = \"test\"  # Required for caching in the decontamination\n",
        "        elif task.has_validation_docs():\n",
        "            task_set = \"val\"  # Required for caching in the decontamination\n",
        "            task_doc_func = task.validation_docs\n",
        "        else:\n",
        "            raise RuntimeError(\"Task has neither test_docs nor validation_docs\")\n",
        "\n",
        "        # deterministically shuffle docs and chop off the first `limit` because sometimes docs are in some kind of order\n",
        "        task_docs = list(task_doc_func())\n",
        "        rnd = random.Random()\n",
        "        rnd.seed(42)\n",
        "        rnd.shuffle(task_docs)\n",
        "\n",
        "        description = (\n",
        "            description_dict[task_name]\n",
        "            if description_dict and task_name in description_dict\n",
        "            else \"\"\n",
        "        )\n",
        "        \n",
        "\n",
        "        for doc_id, doc in enumerate(itertools.islice(task_docs, 0, limit)):\n",
        "\n",
        "            if decontaminate and task.should_decontaminate():\n",
        "                docs_for_decontamination[(task_name, task_set)].append(\n",
        "                    task.doc_to_decontamination_query(doc)\n",
        "                )\n",
        "\n",
        "            docs[(task_name, doc_id)] = doc\n",
        "            ctx = task.fewshot_context(\n",
        "                doc=doc, num_fewshot=num_fewshot, rnd=rnd, description=description\n",
        "            )\n",
        "            reqs = task.construct_requests(doc, ctx)\n",
        "            if not isinstance(reqs, (list, tuple)):\n",
        "                reqs = [reqs]\n",
        "            for i, req in enumerate(reqs):\n",
        "                requests[req.request_type].append(req)\n",
        "                # i: index in requests for a single task instance\n",
        "                # doc_id: unique id that we can get back to a doc using `docs`\n",
        "                requests_origin[req.request_type].append((i, task_name, doc, doc_id))\n",
        "\n",
        "    # Compare all tasks/sets at once to ensure a single training set scan\n",
        "    if decontaminate:\n",
        "        from lm_eval.decontamination.decontaminate import get_train_overlap\n",
        "\n",
        "        print(\"Finding train/test overlap, please wait...\")\n",
        "        overlaps = get_train_overlap(\n",
        "            docs_for_decontamination, decontamination_ngrams_path, limit\n",
        "        )\n",
        "\n",
        "    # all responses for each (task, doc)\n",
        "    process_res_queue = collections.defaultdict(list)\n",
        "\n",
        "    # execute each type of request\n",
        "    for reqtype, reqs in requests.items():\n",
        "        # TODO: right now, this code runs multiple separate LM requests for multiple Requests differing\n",
        "        #       only in index. We could implement some kind of caching, but that would be more of a band-aid\n",
        "        #       solution. we could also implement some kind of auto-grouping here;\n",
        "        #       they should end up next to each other.\n",
        "\n",
        "        print(\"Running\", reqtype, \"requests\")\n",
        "        resps = getattr(lm, reqtype)([req.args for req in reqs])\n",
        "        resps = [\n",
        "            x if req.index is None else x[req.index] for x, req in zip(resps, reqs)\n",
        "        ]\n",
        "        for resp, (i, task_name, doc, doc_id) in zip(resps, requests_origin[reqtype]):\n",
        "            process_res_queue[(task_name, doc_id)].append((i, resp))\n",
        "    vals = collections.defaultdict(list)\n",
        "    \n",
        "    # unpack results and sort back in order and return control to Task\n",
        "    for (task_name, doc_id), requests in process_res_queue.items():\n",
        "        requests.sort(key=lambda x: x[0])\n",
        "        requests = [x[1] for x in requests]\n",
        "\n",
        "        task = task_dict[task_name]\n",
        "        doc = docs[(task_name, doc_id)]\n",
        "\n",
        "        metrics = task.process_results(doc, requests)\n",
        "        for metric, value in metrics.items():\n",
        "            vals[(task_name, metric)].append(value)\n",
        "\n",
        "            # Re-use the evaluation for the decontaminated set by just ignoring the overlaps\n",
        "            if decontaminate and task_name in overlaps:\n",
        "                if doc_id not in overlaps[task_name]:\n",
        "                    vals[(task_name, metric + decontaminate_suffix)].append(value)\n",
        "    \n",
        "\n",
        "    # aggregate results\n",
        "    for (task_name, metric), items in vals.items():\n",
        "        task = task_dict[task_name]\n",
        "        real_metric = metric  # key when looking up the metric with task.aggregation\n",
        "        if metric.endswith(decontaminate_suffix):\n",
        "            real_metric = metric.replace(\n",
        "                decontaminate_suffix, \"\"\n",
        "            )  # decontaminated still uses the same metric\n",
        "        results[task_name][metric] = task.aggregation()[real_metric](items)\n",
        "\n",
        "        # hotfix: bleu, chrf, ter seem to be really expensive to bootstrap\n",
        "        # so we run them less iterations. still looking for a cleaner way to do this\n",
        "\n",
        "        stderr = lm_eval.metrics.stderr_for_metric(\n",
        "            metric=task.aggregation()[real_metric],\n",
        "            bootstrap_iters=min(bootstrap_iters, 1000)\n",
        "            if metric in [\"bleu\", \"chrf\", \"ter\"]\n",
        "            else bootstrap_iters,\n",
        "        )\n",
        "\n",
        "        if stderr is not None:\n",
        "            results[task_name][metric + \"_stderr\"] = stderr(items)\n",
        "\n",
        "    return {\"results\": dict(results), \"versions\": dict(versions)}\n",
        "\n",
        "\n",
        "def make_table(result_dict):\n",
        "    \"\"\"Generate table of results.\"\"\"\n",
        "    from pytablewriter import MarkdownTableWriter, LatexTableWriter\n",
        "\n",
        "    md_writer = MarkdownTableWriter()\n",
        "    latex_writer = LatexTableWriter()\n",
        "    md_writer.headers = [\"Task\", \"Version\", \"Metric\", \"Value\", \"\", \"Stderr\"]\n",
        "    latex_writer.headers = [\"Task\", \"Version\", \"Metric\", \"Value\", \"\", \"Stderr\"]\n",
        "\n",
        "    values = []\n",
        "\n",
        "    for k, dic in result_dict[\"results\"].items():\n",
        "        version = result_dict[\"versions\"][k]\n",
        "        for m, v in dic.items():\n",
        "            if m.endswith(\"_stderr\"):\n",
        "                continue\n",
        "\n",
        "            if m + \"_stderr\" in dic:\n",
        "                se = dic[m + \"_stderr\"]\n",
        "                values.append([k, version, m, \"%.4f\" % v, \"±\", \"%.4f\" % se])\n",
        "            else:\n",
        "                values.append([k, version, m, \"%.4f\" % v, \"\", \"\"])\n",
        "            k = \"\"\n",
        "            version = \"\"\n",
        "    md_writer.value_matrix = values\n",
        "    latex_writer.value_matrix = values\n",
        "\n",
        "    # todo: make latex table look good\n",
        "    # print(latex_writer.dumps())\n",
        "\n",
        "    return md_writer.dumps()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXBBWXMsmCO0",
        "outputId": "16fab842-e478-4a95-fd8e-a9583e0a219d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./lm_eval/evaluator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJRb34raECnB",
        "outputId": "0bb4b8b5-2cd0-4125-c062-5b9a72d42f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './lm_eval/models/qu.py': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm ./lm_eval/models/qu.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3d5Z41tD0Oi",
        "outputId": "69578763-49c0-499c-f72f-2a2a7100478e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./lm_eval/models/qu.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./lm_eval/models/qu.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.module import Module\n",
        "from torch.autograd.function import InplaceFunction\n",
        "import time\n",
        "import copy\n",
        "from transformers.pytorch_utils import Conv1D\n",
        "\n",
        "#from transformers import models\n",
        "#from transformers.models import gpt2\n",
        "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2MLP, GPT2Attention\n",
        "from typing import *\n",
        "\n",
        "from lm_eval import utils\n",
        "from . import gpt2\n",
        "from . import gpt3\n",
        "from . import dummy\n",
        "\n",
        "\n",
        "class new_conv1d(Conv1D):\n",
        "    def __init__(self, cnv, B, E_bits, m):\n",
        "        super().__init__(1,1)\n",
        "\n",
        "        self.E_bits = E_bits\n",
        "        self.B = int(B)\n",
        "        self.m = m\n",
        "\n",
        "        self.training = copy.deepcopy(cnv.training)\n",
        "        self._parameters = copy.deepcopy(cnv._parameters)\n",
        "        self._buffers = copy.deepcopy(cnv._buffers)\n",
        "        self._non_persistent_buffers_set = copy.deepcopy(cnv._non_persistent_buffers_set)\n",
        "        self._backward_hooks = copy.deepcopy(cnv._backward_hooks)\n",
        "        self._is_full_backward_hook = copy.deepcopy(cnv._is_full_backward_hook)\n",
        "        self._forward_hooks = copy.deepcopy(cnv._forward_hooks)\n",
        "        self._forward_pre_hooks = copy.deepcopy(cnv._forward_pre_hooks)\n",
        "        self._state_dict_hooks = copy.deepcopy(cnv._state_dict_hooks)\n",
        "        self._load_state_dict_pre_hooks = copy.deepcopy(cnv._load_state_dict_pre_hooks)\n",
        "        self._load_state_dict_post_hooks = copy.deepcopy(cnv._load_state_dict_post_hooks)\n",
        "        self._modules = copy.deepcopy(cnv._modules)\n",
        "\n",
        "        self.nf = copy.deepcopy(cnv.nf)\n",
        "        t = copy.deepcopy(cnv.weight)\n",
        "        self.weight = torch.nn.Parameter(qunt(t, self.B, self.E_bits, self.m))\n",
        "\n",
        "        s = copy.deepcopy(cnv.bias)\n",
        "        self.bias = torch.nn.Parameter(qunt(s, self.B, self.E_bits, self.m))\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = qunt(x, self.B, self.E_bits, self.m)\n",
        "\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(size_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "class new_embd(nn.Embedding):\n",
        "    def __init__(self, embd, B, E_bits, m):\n",
        "        super().__init__(embd.num_embeddings, embd.embedding_dim)\n",
        "\n",
        "        self.E_bits = E_bits\n",
        "        self.B = int(B)\n",
        "        self.m = m\n",
        "\n",
        "        self.num_embeddings = copy.deepcopy(embd.num_embeddings)\n",
        "        self.embedding_dim = copy.deepcopy(embd.embedding_dim)\n",
        "        self.padding_idx = copy.deepcopy(embd.padding_idx)\n",
        "        self.max_norm = copy.deepcopy(embd.max_norm)\n",
        "        self.norm_type = copy.deepcopy(embd.norm_type)\n",
        "        self.scale_grad_by_freq = copy.deepcopy(embd.scale_grad_by_freq)\n",
        "        t = copy.deepcopy(embd.weight)\n",
        "        self.weight = torch.nn.Parameter(qunt(t, self.B, self.E_bits, self.m))\n",
        "\n",
        "        self.sparse = copy.deepcopy(embd.sparse)\n",
        "\n",
        "\n",
        "        self.training = copy.deepcopy(embd.training)\n",
        "        self._parameters = copy.deepcopy(embd._parameters)\n",
        "        self._buffers = copy.deepcopy(embd._buffers)\n",
        "        self._non_persistent_buffers_set = copy.deepcopy(embd._non_persistent_buffers_set)\n",
        "        self._backward_hooks = copy.deepcopy(embd._backward_hooks)\n",
        "        self._is_full_backward_hook = copy.deepcopy(embd._is_full_backward_hook)\n",
        "        self._forward_hooks = copy.deepcopy(embd._forward_hooks)\n",
        "        self._forward_pre_hooks = copy.deepcopy(embd._forward_pre_hooks)\n",
        "        self._state_dict_hooks = copy.deepcopy(embd._state_dict_hooks)\n",
        "        self._load_state_dict_pre_hooks = copy.deepcopy(embd._load_state_dict_pre_hooks)\n",
        "        self._load_state_dict_post_hooks = copy.deepcopy(embd._load_state_dict_post_hooks)\n",
        "        self._modules = copy.deepcopy(embd._modules)\n",
        "    \n",
        "\n",
        "    def forward(self, input: torch._tensor.Tensor) -> torch._tensor.Tensor:\n",
        "        input = qunt(input, self.B, self.E_bits, self.m)\n",
        "        return F.embedding(\n",
        "            input, self.weight, self.padding_idx, self.max_norm,\n",
        "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
        "\n",
        "\n",
        "class new_gpt2mlp(GPT2MLP):\n",
        "    def __init__(self, mlp, B, E_bits, m):\n",
        "        config = GPT2Config()\n",
        "        inner_dim = config.n_inner if config.n_inner is not None else 4 * config.hidden_size\n",
        "        super().__init__(inner_dim, config)\n",
        "\n",
        "        self.E_bits = E_bits\n",
        "        self.B = int(B)\n",
        "        self.m = m\n",
        "\n",
        "        self.training = copy.deepcopy(mlp.training)\n",
        "        self._parameters = copy.deepcopy(mlp._parameters)\n",
        "        self._buffers = copy.deepcopy(mlp._buffers)\n",
        "        self._non_persistent_buffers_set = copy.deepcopy(mlp._non_persistent_buffers_set)\n",
        "        self._backward_hooks = copy.deepcopy(mlp._backward_hooks)\n",
        "        self._is_full_backward_hook = copy.deepcopy(mlp._is_full_backward_hook)\n",
        "        self._forward_hooks = copy.deepcopy(mlp._forward_hooks)\n",
        "        self._forward_pre_hooks = copy.deepcopy(mlp._forward_pre_hooks)\n",
        "        self._state_dict_hooks = copy.deepcopy(mlp._state_dict_hooks)\n",
        "        self._load_state_dict_pre_hooks = copy.deepcopy(mlp._load_state_dict_pre_hooks)\n",
        "        self._load_state_dict_post_hooks = copy.deepcopy(mlp._load_state_dict_post_hooks)\n",
        "        self._modules = copy.deepcopy(mlp._modules)\n",
        "\n",
        "        self.c_fc = new_conv1d(mlp.c_fc, self.B, self.E_bits, self.m)\n",
        "        self.c_proj = new_conv1d(mlp.c_proj, self.B, self.E_bits, self.m)\n",
        "        self.act = copy.deepcopy(mlp.act)\n",
        "        self.dropout = copy.deepcopy(mlp.dropout)\n",
        "\n",
        "    \n",
        "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "\n",
        "        hidden_states = self.c_fc(hidden_states)\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "\n",
        "        hidden_states = self.act(hidden_states)\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "\n",
        "        hidden_states = self.c_proj(hidden_states)\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "        \n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class new_GPT2Attention(GPT2Attention):\n",
        "    def __init__(self, attn, B, E_bits, m):\n",
        "        super().__init__(GPT2Config())\n",
        "\n",
        "        self.E_bits = E_bits\n",
        "        self.B = int(B)\n",
        "        self.m = m\n",
        "\n",
        "        self.training = copy.deepcopy(attn.training)\n",
        "        self._parameters = copy.deepcopy(attn._parameters)\n",
        "        self._buffers = copy.deepcopy(attn._buffers)\n",
        "        self._non_persistent_buffers_set = copy.deepcopy(attn._non_persistent_buffers_set)\n",
        "        self._backward_hooks = copy.deepcopy(attn._backward_hooks)\n",
        "        self._is_full_backward_hook = copy.deepcopy(attn._is_full_backward_hook)\n",
        "        self._forward_hooks = copy.deepcopy(attn._forward_hooks)\n",
        "        self._forward_pre_hooks = copy.deepcopy(attn._forward_pre_hooks)\n",
        "        self._state_dict_hooks = copy.deepcopy(attn._state_dict_hooks)\n",
        "        self._load_state_dict_pre_hooks = copy.deepcopy(attn._load_state_dict_pre_hooks)\n",
        "        self._load_state_dict_post_hooks = copy.deepcopy(attn._load_state_dict_post_hooks)\n",
        "        self._modules = copy.deepcopy(attn._modules)\n",
        "\n",
        "        self.embed_dim = copy.deepcopy(attn.embed_dim)\n",
        "        self.num_heads = copy.deepcopy(attn.num_heads)\n",
        "        self.head_dim = copy.deepcopy(attn.head_dim)\n",
        "        self.split_size = copy.deepcopy(attn.split_size)\n",
        "        self.scale_attn_weights = copy.deepcopy(attn.scale_attn_weights)\n",
        "        self.is_cross_attention = copy.deepcopy(attn.is_cross_attention)\n",
        "\n",
        "        self.scale_attn_by_inverse_layer_idx = copy.deepcopy(attn.scale_attn_by_inverse_layer_idx)\n",
        "        self.layer_idx = copy.deepcopy(attn.layer_idx)\n",
        "        self.reorder_and_upcast_attn = copy.deepcopy(attn.reorder_and_upcast_attn)\n",
        "\n",
        "        if self.is_cross_attention:\n",
        "            self.c_attn = new_conv1d(copy.deepcopy(attn.c_attn), self.B, self.E_bits, self.m)\n",
        "            self.q_attn = new_conv1d(copy.deepcopy(attn.q_attn), self.B, self.E_bits, self.m)\n",
        "        else:\n",
        "            self.c_attn = new_conv1d(copy.deepcopy(attn.c_attn), self.B, self.E_bits, self.m)\n",
        "        self.c_proj = new_conv1d(copy.deepcopy(attn.c_proj), self.B, self.E_bits, self.m)\n",
        "\n",
        "        self.attn_dropout = copy.deepcopy(attn.attn_dropout)\n",
        "        self.resid_dropout = copy.deepcopy(attn.resid_dropout)\n",
        "        self.pruned_heads = copy.deepcopy(attn.pruned_heads)\n",
        "\n",
        "    \n",
        "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
        "        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
        "\n",
        "        attn_weights = qunt(attn_weights, self.B, self.E_bits, self.m)\n",
        "\n",
        "        if self.scale_attn_weights:\n",
        "            attn_weights = attn_weights / torch.tensor(\n",
        "                value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
        "            )\n",
        "\n",
        "        # Layer-wise attention scaling\n",
        "        if self.scale_attn_by_inverse_layer_idx:\n",
        "            attn_weights = attn_weights / float(self.layer_idx + 1)\n",
        "        \n",
        "        attn_weights = qunt(attn_weights, self.B, self.E_bits, self.m)\n",
        "\n",
        "        if not self.is_cross_attention:\n",
        "            # if only \"normal\" attention layer implements causal mask\n",
        "            query_length, key_length = query.size(-2), key.size(-2)\n",
        "            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\n",
        "            mask_value = torch.finfo(attn_weights.dtype).min\n",
        "            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
        "            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
        "            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n",
        "            attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        attn_weights = qunt(attn_weights, self.B, self.E_bits, self.m)\n",
        "\n",
        "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
        "        attn_weights = attn_weights.type(value.dtype)\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attn_weights = attn_weights * head_mask\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    \n",
        "    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
        "        # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n",
        "        bsz, num_heads, q_seq_len, dk = query.size()\n",
        "        _, _, k_seq_len, _ = key.size()\n",
        "\n",
        "        # Preallocate attn_weights for `baddbmm`\n",
        "        attn_weights = torch.empty(bsz * num_heads, q_seq_len, k_seq_len, dtype=torch.float32, device=query.device)\n",
        "\n",
        "        # Compute Scale Factor\n",
        "        scale_factor = 1.0\n",
        "        if self.scale_attn_weights:\n",
        "            scale_factor /= float(value.size(-1)) ** 0.5\n",
        "\n",
        "        if self.scale_attn_by_inverse_layer_idx:\n",
        "            scale_factor /= float(self.layer_idx + 1)\n",
        "\n",
        "        # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n",
        "        with autocast(enabled=False):\n",
        "            q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n",
        "            attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n",
        "            attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)\n",
        "\n",
        "            attn_weights = qunt(attn_weights, self.B, self.E_bits, self.m)\n",
        "\n",
        "        if not self.is_cross_attention:\n",
        "            # if only \"normal\" attention layer implements causal mask\n",
        "            query_length, key_length = query.size(-2), key.size(-2)\n",
        "            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()\n",
        "            mask_value = torch.finfo(attn_weights.dtype).min\n",
        "            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
        "            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
        "            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n",
        "            attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        attn_weights = qunt(attn_weights, self.B, self.E_bits, self.m)\n",
        "\n",
        "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise\n",
        "        if attn_weights.dtype != torch.float32:\n",
        "            raise RuntimeError(\"Error with upcasting, attn_weights does not have dtype torch.float32\")\n",
        "        attn_weights = attn_weights.type(value.dtype)\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attn_weights = attn_weights * head_mask\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        attn_output = qunt(attn_output, self.B, self.E_bits, self.m)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
        "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
        "\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "\n",
        "        if encoder_hidden_states is not None:\n",
        "            if not hasattr(self, \"q_attn\"):\n",
        "                raise ValueError(\n",
        "                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n",
        "                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n",
        "                )\n",
        "\n",
        "            query = self.q_attn(hidden_states)\n",
        "            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
        "\n",
        "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
        "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
        "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
        "\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past\n",
        "            key = torch.cat((past_key, key), dim=-2)\n",
        "            value = torch.cat((past_value, value), dim=-2)\n",
        "\n",
        "        query = qunt(query, self.B, self.E_bits, self.m)\n",
        "        key = qunt(key, self.B, self.E_bits, self.m)\n",
        "        value = qunt(value, self.B, self.E_bits, self.m)\n",
        "\n",
        "        if use_cache is True:\n",
        "            present = (key, value)\n",
        "        else:\n",
        "            present = None\n",
        "\n",
        "        if self.reorder_and_upcast_attn:\n",
        "            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n",
        "        else:\n",
        "            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
        "\n",
        "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
        "        attn_output = self.c_proj(attn_output)\n",
        "        attn_output = self.resid_dropout(attn_output)\n",
        "\n",
        "        outputs = (attn_output, present)\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs  # a, present, (attentions)\n",
        "\n",
        "\n",
        "class new_GPT2Block(GPT2Block):\n",
        "    def __init__(self, blck, B, E_bits, m):\n",
        "        super().__init__(GPT2Config())\n",
        "\n",
        "        self.training = copy.deepcopy(blck.training)\n",
        "        self._parameters = copy.deepcopy(blck._parameters)\n",
        "        self._buffers = copy.deepcopy(blck._buffers)\n",
        "        self._non_persistent_buffers_set = copy.deepcopy(blck._non_persistent_buffers_set)\n",
        "        self._backward_hooks = copy.deepcopy(blck._backward_hooks)\n",
        "        self._is_full_backward_hook = copy.deepcopy(blck._is_full_backward_hook)\n",
        "        self._forward_hooks = copy.deepcopy(blck._forward_hooks)\n",
        "        self._forward_pre_hooks = copy.deepcopy(blck._forward_pre_hooks)\n",
        "        self._state_dict_hooks = copy.deepcopy(blck._state_dict_hooks)\n",
        "        self._load_state_dict_pre_hooks = copy.deepcopy(blck._load_state_dict_pre_hooks)\n",
        "        self._load_state_dict_post_hooks = copy.deepcopy(blck._load_state_dict_post_hooks)\n",
        "        self._modules = copy.deepcopy(blck._modules)\n",
        "\n",
        "        self.E_bits = E_bits\n",
        "        self.B = int(B)\n",
        "        self.m = m\n",
        "\n",
        "        self.ln_1 = copy.deepcopy(blck.ln_1)\n",
        "        self.attn = new_GPT2Attention(blck.attn, self.B, self.E_bits, self.m)\n",
        "        \n",
        "        self.ln_2 = Module()\n",
        "        self.ln_2 = copy.deepcopy(blck.ln_2)\n",
        "\n",
        "        try:\n",
        "            self.crossattention = new_GPT2Attention(blck.crossattention, self.B, self.E_bits, self.m)\n",
        "            self.ln_cross_attn = copy.deepcopy(blck.ln_cross_attn)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        self.mlp = new_gpt2mlp(blck.mlp, self.B, self.E_bits, self.m)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
        "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_1(hidden_states)\n",
        "\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "\n",
        "        attn_outputs = self.attn(\n",
        "            hidden_states,\n",
        "            layer_past=layer_past,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
        "\n",
        "        outputs = attn_outputs[1:]\n",
        "        # residual connection\n",
        "        hidden_states = attn_output + residual\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "\n",
        "        if encoder_hidden_states is not None:\n",
        "            # add one self-attention block for cross-attention\n",
        "            if not hasattr(self, \"crossattention\"):\n",
        "                raise ValueError(\n",
        "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
        "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "                )\n",
        "            residual = hidden_states\n",
        "            hidden_states = self.ln_cross_attn(hidden_states)\n",
        "\n",
        "            cross_attn_outputs = self.crossattention(\n",
        "                hidden_states,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                encoder_hidden_states=encoder_hidden_states,\n",
        "                encoder_attention_mask=encoder_attention_mask,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            attn_output = cross_attn_outputs[0]\n",
        "\n",
        "            # residual connection\n",
        "            hidden_states = residual + attn_output\n",
        "\n",
        "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_2(hidden_states)\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "\n",
        "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
        "        # residual connection\n",
        "        hidden_states = residual + feed_forward_hidden_states\n",
        "        hidden_states = qunt(hidden_states, self.B, self.E_bits, self.m)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs = (hidden_states,) + outputs\n",
        "        else:\n",
        "            outputs = (hidden_states,) + outputs[1:]\n",
        "\n",
        "        return outputs  # hidden_states, present, (attentions, cross_attentions)\n",
        "\n",
        "\n",
        "class qu(gpt2.GPT2LM):\n",
        "    def __init__(self, B, *args, **kwargs):\n",
        "        super(qu, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.E_bits = 2\n",
        "        self.B = int(B)\n",
        "        self.m = 5\n",
        "\n",
        "        self.E = 2**self.E_bits\n",
        "\n",
        "        self.quantizeFwd = True\n",
        "\n",
        "        # update wte\n",
        "        self.gpt2.transformer._modules[\"old_wte\"]=copy.deepcopy(self.gpt2.transformer._modules[\"wte\"])\n",
        "        self.gpt2.transformer._modules[\"wte\"] = new_embd(self.gpt2.transformer.old_wte, self.B, self.E_bits, self.m)\n",
        "       \n",
        "        # update wpe\n",
        "        self.gpt2.transformer._modules[\"old_wpe\"]=copy.deepcopy(self.gpt2.transformer._modules[\"wpe\"])\n",
        "        self.gpt2.transformer._modules[\"wpe\"] = new_embd(self.gpt2.transformer.old_wpe, self.B, self.E_bits, self.m)\n",
        "\n",
        "        # updae h\n",
        "        for i in range(len(self.gpt2.transformer.h)):\n",
        "            mo = self.gpt2.transformer.h[i]\n",
        "            new_m = new_GPT2Block(mo, self.B, self.E_bits, self.m)\n",
        "            self.gpt2.transformer.h[i] = new_m\n",
        "    \n",
        "\n",
        "    def new_wte(self, input):\n",
        "          input = qunt(input, self.B, self.E_bits, self.m)\n",
        "          return self.gpt2.transformer.old_wte(input)\n",
        "        \n",
        "\n",
        "def qunt(input, B, E_bits, m):\n",
        "    isint = (input.type()==input.int().type())\n",
        "    islong = (input.type()==input.long().type())\n",
        "\n",
        "    # To positive\n",
        "    sign = (input.__ge__(0).int()-0.5)*2\n",
        "    abs_input = input * sign\n",
        "\n",
        "    # Upper clipping\n",
        "    Max = (2 - 2 ** (-m)) * (2 ** (2 ** E_bits - B))\n",
        "    abs_input = torch.clip(abs_input, max=Max)\n",
        "\n",
        "    # Lower clipping (just for exponent classes), for avoiding log(0) and log(<=0.5)\n",
        "    abs_input_without_0 = torch.clip(abs_input, min=2 ** (1-B))\n",
        "\n",
        "    # Exponent classes\n",
        "    exp_range = torch.log2(abs_input_without_0)\n",
        "    exp_range = exp_range+B\n",
        "    exp_range = exp_range.int()\n",
        "    exp_range = exp_range.float()\n",
        "    exp_range = exp_range-B\n",
        "\n",
        "    # 1 if is not in the smaller exponent-class (union of 0 and 1)\n",
        "    min_mask = torch.ge(exp_range, 1.5 - B).int()\n",
        "\n",
        "    # Maximum value to include in this exponent class\n",
        "    upper_bound = 2 ** (exp_range + 1)\n",
        "\n",
        "    # Minimum value to include in this exponent class\n",
        "    lower_bound = 2 ** exp_range\n",
        "    lower_bound = lower_bound * min_mask\n",
        "\n",
        "    # Number of values in class\n",
        "    num_of_value = 2**m\n",
        "    num_of_values = (2 - min_mask) * num_of_value\n",
        "\n",
        "    abs_input = abs_input - lower_bound\n",
        "    abs_input = abs_input * num_of_values\n",
        "    abs_input = abs_input / (upper_bound - lower_bound)\n",
        "\n",
        "\n",
        "    abs_input = abs_input + 0.5\n",
        "    abs_input = abs_input.int()\n",
        "    abs_input = abs_input * (upper_bound - lower_bound)\n",
        "    abs_input = abs_input / num_of_values\n",
        "    abs_input = abs_input + lower_bound\n",
        "    \n",
        "    x = abs_input * sign\n",
        "    if isint:\n",
        "      x = x.int()\n",
        "    if islong:\n",
        "      x = x.long()\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pa6TJCizTj8"
      },
      "outputs": [],
      "source": [
        "!rm ./lm_eval/models/__init__.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdG0Tlw1znrT",
        "outputId": "30832136-c549-4fd5-f0aa-9c5d426e35f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./lm_eval/models/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./lm_eval/models/__init__.py\n",
        "from . import gpt2\n",
        "from . import gpt3\n",
        "from . import dummy\n",
        "\n",
        "from . import qu\n",
        "\n",
        "MODEL_REGISTRY = {\n",
        "    \"qu\": qu.qu,\n",
        "    \"gpt2\": gpt2.GPT2LM,\n",
        "}\n",
        "\n",
        "\n",
        "def get_model(model_name):\n",
        "    return MODEL_REGISTRY[model_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjDa1zY51wxB"
      },
      "outputs": [],
      "source": [
        "!rm ./main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYLrqcoJ16TN",
        "outputId": "73be5ae8-d6d0-4155-ab21-601101ab2ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./main.py\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import fnmatch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from lm_eval import tasks, evaluator\n",
        "\n",
        "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "class MultiChoice:\n",
        "    def __init__(self, choices):\n",
        "        self.choices = choices\n",
        "\n",
        "    # Simple wildcard support (linux filename patterns)\n",
        "    def __contains__(self, values):\n",
        "        for value in values.split(\",\"):\n",
        "            if len(fnmatch.filter(self.choices, value)) == 0:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def __iter__(self):\n",
        "        for choice in self.choices:\n",
        "            yield choice\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", required=True)\n",
        "    parser.add_argument(\"--model_args\", default=\"\")\n",
        "    parser.add_argument(\"--tasks\", default=None, choices=MultiChoice(tasks.ALL_TASKS))\n",
        "    parser.add_argument(\"--provide_description\", action=\"store_true\")\n",
        "    parser.add_argument(\"--num_fewshot\", type=int, default=0)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=None)\n",
        "    parser.add_argument(\"--device\", type=str, default=None)\n",
        "    parser.add_argument(\"--output_path\", default=None)\n",
        "    parser.add_argument(\"--limit\", type=int, default=None)\n",
        "    parser.add_argument(\"--no_cache\", action=\"store_true\")\n",
        "    parser.add_argument(\"--decontamination_ngrams_path\", default=None)\n",
        "    parser.add_argument(\"--description_dict_path\", default=None)\n",
        "    parser.add_argument(\"--check_integrity\", action=\"store_true\")\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "# Returns a list containing all values of the source_list that\n",
        "# match at least one of the patterns\n",
        "def pattern_match(patterns, source_list):\n",
        "    task_names = set()\n",
        "    for pattern in patterns:\n",
        "        for matching in fnmatch.filter(source_list, pattern):\n",
        "            task_names.add(matching)\n",
        "    return list(task_names)\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    assert not args.provide_description  # not implemented\n",
        "\n",
        "    if args.limit:\n",
        "        print(\n",
        "            \"WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\"\n",
        "        )\n",
        "\n",
        "    if args.tasks is None:\n",
        "        task_names = tasks.ALL_TASKS\n",
        "    else:\n",
        "        task_names = pattern_match(args.tasks.split(\",\"), tasks.ALL_TASKS)\n",
        "\n",
        "    print(f\"Selected Tasks: {task_names}\")\n",
        "\n",
        "    description_dict = {}\n",
        "    if args.description_dict_path:\n",
        "        with open(args.description_dict_path, \"r\") as f:\n",
        "            description_dict = json.load(f)\n",
        "\n",
        "    R = dict()\n",
        "\n",
        "    _from, _to = -1, 2\n",
        "\n",
        "    for i in range(_from, _to):\n",
        "        print(f\"B = {i}:\")\n",
        "        results = evaluator.simple_evaluate(\n",
        "            model=args.model,\n",
        "            model_args=f\"B={i}\",\n",
        "            tasks=task_names,\n",
        "            num_fewshot=args.num_fewshot,\n",
        "            batch_size=args.batch_size,\n",
        "            device=args.device,\n",
        "            no_cache=args.no_cache,\n",
        "            limit=args.limit,\n",
        "            description_dict=description_dict,\n",
        "            decontamination_ngrams_path=args.decontamination_ngrams_path,\n",
        "            check_integrity=args.check_integrity,\n",
        "        )\n",
        "        results = results['results']\n",
        "        for K in results.keys():\n",
        "            for k in results[K].keys():\n",
        "                if \"_stderr\" not in k:\n",
        "                    if f\"{K}: {k}\" not in R.keys():\n",
        "                        R[f\"{K}: {k}\"] = []\n",
        "                    R[f\"{K}: {k}\"].append(results[K][k])\n",
        "                    print(f\"{K}: {k}: {results[K][k]}\")\n",
        "\n",
        "    _keys = list(R.keys())\n",
        "    for k in _keys:\n",
        "        print(f\"{k}: {R[k]}\")\n",
        "        plt.plot(range(_from, _to), R[k], label=k)\n",
        "    plt.legend(_keys)\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFwSgJpC1_LH",
        "outputId": "abffcfab-18bc-49c9-c40c-273744bbe6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Tasks: ['hellaswag']\n",
            "B = -1:\n",
            "Using device '0'\n",
            "#####\n",
            "WARNING:datasets.builder:Found cached dataset hellaswag (/root/.cache/huggingface/datasets/hellaswag/default/0.1.0/c37cd37196278995f42bc32f532730ae9b0d5f0f4a2d3b97735c17ff3ad67169)\n",
            "100% 3/3 [00:00<00:00, 752.03it/s]\n",
            "Running loglikelihood requests\n",
            "100% 40145/40145 [1:31:29<00:00,  7.31it/s]\n",
            "G2\n",
            "G5\n",
            "G6\n",
            "hellaswag: acc: 0.26020713005377416\n",
            "hellaswag: acc_norm: 0.2544313881696873\n",
            "B = 0:\n",
            "Using device '0'\n",
            "#####\n",
            "WARNING:datasets.builder:Found cached dataset hellaswag (/root/.cache/huggingface/datasets/hellaswag/default/0.1.0/c37cd37196278995f42bc32f532730ae9b0d5f0f4a2d3b97735c17ff3ad67169)\n",
            "100% 3/3 [00:00<00:00, 692.66it/s]\n",
            "Running loglikelihood requests\n",
            "  0% 173/40145 [00:24<1:35:49,  6.95it/s]Exception ignored in: <generator object tqdm.__iter__ at 0x7fea77c00cd0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tqdm/std.py\", line 1210, in __iter__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tqdm/std.py\", line 1289, in close\n",
            "    self._decr_instances(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tqdm/std.py\", line 595, in _decr_instances\n",
            "    cls._instances.remove(instance)\n",
            "  File \"/usr/lib/python3.7/_weakrefset.py\", line 109, in remove\n",
            "    self.data.remove(ref(item))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tqdm/utils.py\", line 74, in __eq__\n",
            "    def __eq__(self, other):\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 117, in <module>\n",
            "    main()\n",
            "  File \"main.py\", line 98, in main\n",
            "    check_integrity=args.check_integrity,\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/utils.py\", line 162, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/evaluator.py\", line 92, in simple_evaluate\n",
            "    decontamination_ngrams_path=decontamination_ngrams_path,\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/utils.py\", line 162, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/evaluator.py\", line 247, in evaluate\n",
            "    resps = getattr(lm, reqtype)([req.args for req in reqs])\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/base.py\", line 820, in fn\n",
            "    rem_res = getattr(self.lm, attr)(remaining_reqs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/base.py\", line 185, in loglikelihood\n",
            "    return self._loglikelihood_tokens(new_reqs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/base.py\", line 295, in _loglikelihood_tokens\n",
            "    self._model_call(batched_inps), dim=-1\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/models/gpt2.py\", line 122, in _model_call\n",
            "    return self.gpt2(inps)[0][:, :, :50257]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1059, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 833, in forward\n",
            "    position_embeds = self.wpe(position_ids)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/models/qu.py\", line 97, in forward\n",
            "    input = qunt(input, self.B, self.E_bits, self.m)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/models/qu.py\", line 545, in qunt\n",
            "    num_of_value = 2**m\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python main.py \\\n",
        "\t--model qu \\\n",
        "\t--device 0 \\\n",
        "\t--tasks hellaswag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7pZWScF2t_o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}